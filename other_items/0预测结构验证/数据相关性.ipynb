{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "423eb7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu121\n",
      "12.1\n",
      "NVIDIA GeForce RTX 4070 SUPER\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70825c32",
   "metadata": {},
   "source": [
    "# æŸ¥çœ‹é¢„æµ‹ç›®æ ‡å†…éƒ¨å…³ç³»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88c0c00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Software\\conda\\envs\\kaggle2\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“¦\n",
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import h5py\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "import socket\n",
    "import psutil\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tabulate import tabulate\n",
    "\n",
    "from sklearn.model_selection import KFold, GroupKFold\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.amp import autocast, GradScaler       \n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image\n",
    "\n",
    "import timm\n",
    "from torchvision import models, transforms\n",
    "from torchvision.models import get_model_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f331799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… file path ï¼š\n",
      "dir          : D:\\DATA_hao\\Kaggle_\\csiro-biomass\n",
      "train        : D:\\DATA_hao\\Kaggle_\\csiro-biomass\\train\n",
      "test         : D:\\DATA_hao\\Kaggle_\\csiro-biomass\\test\n",
      "model        : D:\\DATA_hao\\Kaggle_\\csiro-biomass\\DualStream_multihead\n",
      "data         : D:\\DATA_hao\\Kaggle_\\csiro-biomass\n"
     ]
    }
   ],
   "source": [
    "# ğŸŒ± Path Initialization\n",
    "if socket.gethostname() == 'hao-2':\n",
    "    dir = Path('D:/DATA_hao/Kaggle_/csiro-biomass/')\n",
    "    DIRS = {\n",
    "        \"dir\"  : dir,\n",
    "        \"train\": Path(dir, \"train\"),\n",
    "        \"test\" : Path(dir, \"test\"),\n",
    "        \"model\": Path(dir, \"DualStream_multihead\"),\n",
    "        \"data\" : Path(dir),\n",
    "    }\n",
    "\n",
    "\n",
    "    print(\"âœ… file path ï¼š\")\n",
    "    for key, path in DIRS.items():\n",
    "        print(f\"{key:<12} : {path}\")\n",
    "\n",
    "elif socket.gethostname() == 'user-PowerEdge-XE9680':\n",
    "    dir = Path('/data4/huangweigang/gh/csiro-biomass')\n",
    "    DIRS = {\n",
    "        \"dir\"  : dir,\n",
    "        \"train\": Path(dir, \"train\"),\n",
    "        \"test\" : Path(dir, \"test\"),\n",
    "        \"model\": Path(dir, \"DualStream_multihead\"),\n",
    "        \"data\" : Path(dir),\n",
    "    }\n",
    "\n",
    "\n",
    "    print(\"âœ… file path ï¼š\")\n",
    "    for key, path in DIRS.items():\n",
    "        print(f\"{key:<12} : {path}\")\n",
    "\n",
    "else:\n",
    "    dir = Path('/kaggle/input/csiro-biomass')\n",
    "    DIRS = {\n",
    "        \"dir\"  : dir,\n",
    "        \"train\": Path(dir, \"train\"),\n",
    "        \"test\" : Path(dir, \"test\"),\n",
    "        \"model\": Path('/kaggle/input', \"dualstream-multihead2025-11-04-02-07-11\"),\n",
    "        \"data\" : Path(\"/kaggle/working/\"),\n",
    "    }\n",
    "\n",
    "\n",
    "    print(\"âœ… file path ï¼š\")\n",
    "    for key, path in DIRS.items():\n",
    "        print(f\"{key:<12} : {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbeaa568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def show_df_info(df, name: str):\n",
    "    \"\"\"\n",
    "    Print the shape and column names of a single DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df   : pandas.DataFrame\n",
    "        name : Display name (string)\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ“Š {name:<16} shape: {str(df.shape):<16}  columns: {df.columns.tolist()}\")\n",
    "\n",
    "def select_free_gpu(threshold_mem_MB = 500, threshold_util = 20):\n",
    "    \"\"\"\n",
    "    Automatically select an available GPU (works in both .py and Jupyter environments).\n",
    "    \"\"\"\n",
    "\n",
    "    # === Internal function: query GPU info from nvidia-smi ===\n",
    "    def get_gpu_info():\n",
    "        \"\"\"Retrieve GPU information using nvidia-smi.\"\"\"\n",
    "        query = (\n",
    "            \"index,name,memory.used,memory.total,utilization.gpu,temperature.gpu,power.draw\"\n",
    "        )\n",
    "        result = subprocess.run(\n",
    "            [\"nvidia-smi\", f\"--query-gpu={query}\", \"--format=csv,noheader,nounits\"],\n",
    "            capture_output=True, text=True\n",
    "        )\n",
    "\n",
    "        gpus = []\n",
    "        for line in result.stdout.strip().split(\"\\n\"):\n",
    "            idx, name, mem_used, mem_total, util, temp, power = [x.strip() for x in line.split(\",\")]\n",
    "            gpus.append({\n",
    "                \"index\": int(idx),\n",
    "                \"name\": name,\n",
    "                \"mem_used_MB\": int(mem_used),\n",
    "                \"mem_total_MB\": int(mem_total),\n",
    "                \"util_%\": int(util),\n",
    "                \"temp_C\": int(temp),\n",
    "                \"power_W\": float(power),\n",
    "            })\n",
    "        return gpus\n",
    "\n",
    "    # === Main logic ===\n",
    "    gpus = get_gpu_info()\n",
    "\n",
    "    # Select GPUs with low memory and utilization\n",
    "    free_gpus = [\n",
    "        g for g in gpus\n",
    "        if g[\"mem_used_MB\"] < threshold_mem_MB and g[\"util_%\"] <= threshold_util\n",
    "    ]\n",
    "\n",
    "    if not free_gpus:\n",
    "        gpus.sort(key=lambda x: x[\"mem_used_MB\"])\n",
    "        selected = gpus[0]\n",
    "        reason = \"(No fully idle GPU found â€” selected the one with lowest memory usage)\"\n",
    "    else:\n",
    "        selected = free_gpus[0]\n",
    "        reason = \"(Idle GPU detected)\"\n",
    "\n",
    "    # Print GPU information table\n",
    "    print(tabulate(\n",
    "        [[g[\"index\"], g[\"name\"], f\"{g['mem_used_MB']}/{g['mem_total_MB']} MB\",\n",
    "          f\"{g['util_%']}%\", f\"{g['temp_C']}Â°C\", f\"{g['power_W']}W\"]\n",
    "         for g in gpus],\n",
    "        headers=[\"GPU\", \"Name\", \"Memory\", \"Util\", \"Temp\", \"Power\"],\n",
    "        tablefmt=\"grid\"\n",
    "    ))\n",
    "\n",
    "    idx = selected[\"index\"]\n",
    "    device_name = f\"cuda:{idx}\"\n",
    "\n",
    "    # Detect if running inside a Jupyter Notebook\n",
    "    try:\n",
    "        from IPython import get_ipython\n",
    "        in_notebook = get_ipython() is not None\n",
    "    except Exception:\n",
    "        in_notebook = False\n",
    "\n",
    "    if not in_notebook:\n",
    "        # âœ… Safe to set environment variable in normal Python script\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(idx)\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"\\nâœ… Selected GPU {idx} {reason}\")\n",
    "        print(f\"Current device: {device} (logical GPU {idx})\\n\")\n",
    "    else:\n",
    "        # âš ï¸ In notebook environments, do not modify environment variables\n",
    "        device = torch.device(device_name if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"\\nâš ï¸ Detected Jupyter environment â€” not modifying CUDA_VISIBLE_DEVICES.\")\n",
    "        print(f\"âœ… Using device: {device_name} {reason}\\n\")\n",
    "\n",
    "    return idx, device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8c29a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess training data\n",
    "def load_and_prepare_train_df():\n",
    "    # 1ï¸âƒ£ Read raw CSV file\n",
    "    df_file_path = Path(DIRS[\"dir\"]) / \"train.csv\"\n",
    "    df = pd.read_csv(df_file_path)\n",
    "\n",
    "    # 2ï¸âƒ£ Extract unique ID (e.g., \"ID1011485656__Dry_Green_g\" â†’ \"ID1011485656\")\n",
    "    df[\"ID\"] = df[\"sample_id\"].str.split(\"__\").str[0]\n",
    "\n",
    "    # 3ï¸âƒ£ Move ID column to the front\n",
    "    df = df[[\"ID\"] + [c for c in df if c != \"ID\"]]\n",
    "\n",
    "    # 4ï¸âƒ£ Pivot target values (long â†’ wide format)\n",
    "    df_targets = (\n",
    "        df.pivot_table(\n",
    "            index=\"ID\",\n",
    "            columns=\"target_name\",\n",
    "            values=\"target\",\n",
    "            aggfunc=\"first\"\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "    df_targets.columns.name = None  # remove multi-index column names\n",
    "\n",
    "    # 5ï¸âƒ£ Extract metadata (one row per ID)\n",
    "    meta_cols = [\n",
    "        \"ID\", \"image_path\", \"Sampling_Date\", \"State\",\n",
    "        \"Species\", \"Pre_GSHH_NDVI\", \"Height_Ave_cm\"\n",
    "    ]\n",
    "    df_meta = df[meta_cols].drop_duplicates(subset=\"ID\")\n",
    "\n",
    "    # 6ï¸âƒ£ Merge metadata with target values\n",
    "    df_train = pd.merge(df_meta, df_targets, on=\"ID\", how=\"left\")\n",
    "    show_df_info(df_train, \"df_train\")\n",
    "\n",
    "    return df_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5ac761",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "746fe813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š df_train         shape: (357, 12)         columns: ['ID', 'image_path', 'Sampling_Date', 'State', 'Species', 'Pre_GSHH_NDVI', 'Height_Ave_cm', 'Dry_Clover_g', 'Dry_Dead_g', 'Dry_Green_g', 'Dry_Total_g', 'GDM_g']\n"
     ]
    }
   ],
   "source": [
    "# è¯»å–è®­ç»ƒæ•°æ®\n",
    "df = load_and_prepare_train_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac125c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# å…³ç³»1ï¼šDry_Total_g\n",
    "df[\"Dry_Total_pred\"] = (\n",
    "    df[\"Dry_Clover_g\"] + \n",
    "    df[\"Dry_Dead_g\"] + \n",
    "    df[\"Dry_Green_g\"]\n",
    ")\n",
    "df[\"err_total\"] = (df[\"Dry_Total_pred\"] - df[\"Dry_Total_g\"]).abs()\n",
    "\n",
    "# å…³ç³»2ï¼šGDM_g\n",
    "df[\"GDM_pred\"] = (\n",
    "    df[\"Dry_Dead_g\"] + \n",
    "    df[\"Dry_Green_g\"]\n",
    ")\n",
    "df[\"err_gdm\"] = (df[\"GDM_pred\"] - df[\"GDM_g\"]).abs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56fbcd5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š df_train         shape: (357, 12)         columns: ['ID', 'image_path', 'Sampling_Date', 'State', 'Species', 'Pre_GSHH_NDVI', 'Height_Ave_cm', 'Dry_Clover_g', 'Dry_Dead_g', 'Dry_Green_g', 'Dry_Total_g', 'GDM_g']\n",
      "\n",
      "Totalï¼š\n",
      "err_total_group\n",
      "<0.001        356\n",
      "0.1~1           1\n",
      "0.001~0.01      0\n",
      "0.01~0.1        0\n",
      "1~10            0\n",
      ">10             0\n",
      "Name: count, dtype: int64\n",
      "\n",
      "GDMï¼š\n",
      "err_gdm_group\n",
      "<0.001        357\n",
      "0.001~0.01      0\n",
      "0.01~0.1        0\n",
      "0.1~1           0\n",
      "1~10            0\n",
      ">10             0\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = load_and_prepare_train_df()\n",
    "\n",
    "df[\"Dry_Total_pred\"] = (\n",
    "    df[\"Dry_Clover_g\"] + \n",
    "    df[\"Dry_Dead_g\"] + \n",
    "    df[\"Dry_Green_g\"]\n",
    ")\n",
    "df[\"err_total\"] = (df[\"Dry_Total_pred\"] - df[\"Dry_Total_g\"]).abs()\n",
    "\n",
    "\n",
    "\n",
    "df[\"GDM_pred\"] = (\n",
    "    df[\"Dry_Clover_g\"] + \n",
    "    df[\"Dry_Green_g\"]\n",
    ")\n",
    "df[\"err_gdm\"] = (df[\"GDM_pred\"] - df[\"GDM_g\"]).abs()\n",
    "\n",
    "\n",
    "\n",
    "bins = [0, 0.001, 0.01, 0.1, 1, 10, np.inf]\n",
    "labels = [\"<0.001\", \"0.001~0.01\", \"0.01~0.1\", \"0.1~1\", \"1~10\", \">10\"]\n",
    "\n",
    "\n",
    "\n",
    "df[\"err_total_group\"] = pd.cut(df[\"err_total\"], bins=bins, labels=labels, right=False)\n",
    "df[\"err_gdm_group\"] = pd.cut(df[\"err_gdm\"], bins=bins, labels=labels, right=False)\n",
    "\n",
    "\n",
    "print(\"\\nTotalï¼š\")\n",
    "print(df[\"err_total_group\"].value_counts())\n",
    "\n",
    "print(\"\\nGDMï¼š\")\n",
    "print(df[\"err_gdm_group\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754eadbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26b2daf4",
   "metadata": {},
   "source": [
    "# æœ€ç»ˆæŠ¥å‘Šåˆ›å»ºæ–°ç‰¹å¾\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237da2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "# =====================================================\n",
    "# ğŸ§© Step 1: ç‰¹å¾å¢å¼º\n",
    "# =====================================================\n",
    "def enrich_features(df: pd.DataFrame):\n",
    "    df = df.copy()\n",
    "    df[\"NDVI\"] = df[\"Pre_GSHH_NDVI\"]\n",
    "    df[\"Height\"] = df[\"Height_Ave_cm\"]\n",
    "\n",
    "    # --- NDVI Ã— Height ç³»ç‰¹å¾ ---\n",
    "    df[\"NDVI_H\"] = df[\"NDVI\"] * df[\"Height\"]\n",
    "    df[\"NDVI_logH\"] = df[\"NDVI\"] * np.log1p(df[\"Height\"])\n",
    "\n",
    "    # --- éçº¿æ€§æ‰©å±• ---\n",
    "    df[\"NDVI2\"] = df[\"NDVI\"] ** 2\n",
    "    df[\"Height2\"] = df[\"Height\"] ** 2\n",
    "    df[\"logH\"] = np.log1p(df[\"Height\"])\n",
    "\n",
    "    # --- æ—¶é—´ç‰¹å¾ ---\n",
    "    df[\"month\"] = pd.to_datetime(df[\"Sampling_Date\"], errors=\"coerce\").dt.month\n",
    "    df[\"season_sin\"] = np.sin(2 * np.pi * df[\"month\"] / 12)\n",
    "    df[\"season_cos\"] = np.cos(2 * np.pi * df[\"month\"] / 12)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# ğŸŒ¿ Step 2: ç‰©ç§æ ‡å‡†åŒ–ï¼ˆå…¨éƒ¨å°å†™ + åˆå¹¶åŒç±»ï¼‰\n",
    "# =====================================================\n",
    "def normalize_species(df, col=\"Species\"):\n",
    "    df = df.copy()\n",
    "    df[col] = df[col].astype(str).str.lower().str.strip()\n",
    "\n",
    "    replacements = {\n",
    "        # Clover ç³»\n",
    "        \"whiteclover\": \"clover\",\n",
    "        \"subcloverdalkeith\": \"clover\",\n",
    "        \"subcloverlosa\": \"clover\",\n",
    "        \"clover\": \"clover\",\n",
    "\n",
    "        # Ryegrass ç³»\n",
    "        \"ryegrass\": \"ryegrass\",\n",
    "        \"barleygrass\": \"ryegrass\",\n",
    "        \"bromegrass\": \"ryegrass\",\n",
    "        \"barleygrass_ryegrass\": \"ryegrass\",\n",
    "\n",
    "        # å…¶ä»–ä¸»ç±»\n",
    "        \"phalaris\": \"phalaris\",\n",
    "        \"fescue\": \"fescue\",\n",
    "        \"lucerne\": \"lucerne\",\n",
    "\n",
    "        # æ‚è‰ç±»ç»Ÿä¸€\n",
    "        \"silvergrass\": \"weed\",\n",
    "        \"speargrass\": \"weed\",\n",
    "        \"capeweed\": \"weed\",\n",
    "        \"crumbweed\": \"weed\",\n",
    "        \"mixed\": \"weed\",\n",
    "    }\n",
    "\n",
    "    def map_species(name):\n",
    "        for k, v in replacements.items():\n",
    "            if re.search(k, name):\n",
    "                return v\n",
    "        return name\n",
    "\n",
    "    df[\"Species_norm\"] = df[col].apply(map_species)\n",
    "    return df\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# ğŸ“Š Step 3: è®¡ç®—ç›¸å…³æ€§ï¼ˆæ•´ä½“ + æŒ‰ç‰©ç§ï¼‰\n",
    "# =====================================================\n",
    "def analyze_correlations(df):\n",
    "    targets = [\"Dry_Clover_g\", \"Dry_Dead_g\", \"Dry_Green_g\", \"Dry_Total_g\", \"GDM_g\"]\n",
    "    feats = [\n",
    "        \"NDVI\", \"Height\", \"logH\",\n",
    "        \"NDVI_H\", \"NDVI_logH\",\n",
    "        \"NDVI2\", \"Height2\",\n",
    "        \"season_sin\", \"season_cos\"\n",
    "    ]\n",
    "\n",
    "    df_log = df.copy()\n",
    "    df_log[targets] = np.log1p(df_log[targets])\n",
    "\n",
    "    corr_all = df_log[feats + targets].corr().loc[feats, targets]\n",
    "    print(\"\\nâœ… å…¨å±€ç‰¹å¾â€“ç›®æ ‡ Pearson ç›¸å…³æ€§ï¼ˆlog1p å˜æ¢ï¼‰ï¼š\")\n",
    "    display(corr_all)\n",
    "\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    sns.heatmap(corr_all, annot=True, fmt=\".2f\", cmap=\"YlGnBu\")\n",
    "    plt.title(\"Global Featureâ€“Target Correlations (log1p targets)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # æŒ‰ç‰©ç§åˆ†ç»„ç›¸å…³æ€§\n",
    "    species_corrs = {}\n",
    "    for sp, sub in df_log.groupby(\"Species_norm\"):\n",
    "        if len(sub) < 10:\n",
    "            continue\n",
    "        corrs = sub[feats + targets].corr().loc[feats, targets]\n",
    "        species_corrs[sp] = corrs\n",
    "    return corr_all, species_corrs\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# ğŸ“ˆ Step 4: æ±‡æ€»ä¸å¹³å‡æ’åè¾“å‡º\n",
    "# =====================================================\n",
    "def summarize_species_corrs(species_corrs):\n",
    "    mean_corrs = pd.DataFrame({\n",
    "        sp: mat[\"Dry_Total_g\"] for sp, mat in species_corrs.items()\n",
    "    }).T.sort_values(by=\"NDVI_logH\", ascending=False)\n",
    "    print(\"\\nğŸ“Š å„ç‰©ç§å¯¹ Dry_Total_g çš„ç›¸å…³æ€§ï¼ˆæŒ‰ NDVI_logH æ’åºï¼‰ï¼š\")\n",
    "    display(mean_corrs)\n",
    "    plt.figure(figsize=(8,5))\n",
    "    sns.heatmap(mean_corrs, annot=True, cmap=\"YlOrBr\", fmt=\".2f\")\n",
    "    plt.title(\"Per-species Featureâ€“Dry_Total_g Correlations (log1p)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return mean_corrs\n",
    "\n",
    "\n",
    "def run_full_analysis(df_raw):\n",
    "    print(\"ğŸ”§ ç”Ÿæˆå¢å¼ºç‰¹å¾ ...\")\n",
    "    df = enrich_features(df_raw)\n",
    "    print(\"ğŸŒ¿ ç»Ÿä¸€ç‰©ç§åç§° ...\")\n",
    "    # df = normalize_species(df)\n",
    "\n",
    "    # æ˜¾ç¤ºç‰©ç§åˆ†å¸ƒ\n",
    "    species_counts = df[\"Species_norm\"].value_counts().reset_index()\n",
    "    species_counts.columns = [\"Species\", \"Count\"]\n",
    "    print(\"\\nğŸ“Š ç‰©ç§åˆ†å¸ƒï¼š\")\n",
    "    display(species_counts)\n",
    "\n",
    "    # ç›¸å…³æ€§åˆ†æ\n",
    "    corr_all, species_corrs = analyze_correlations(df)\n",
    "\n",
    "    # æ±‡æ€»ç‰©ç§å¯¹ Dry_Total_g çš„ç›¸å…³æ€§\n",
    "    mean_corrs = summarize_species_corrs(species_corrs)\n",
    "    return df, corr_all, species_corrs, mean_corrs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04adc091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š df_train         shape: (357, 12)         columns: ['ID', 'image_path', 'Sampling_Date', 'State', 'Species', 'Pre_GSHH_NDVI', 'Height_Ave_cm', 'Dry_Clover_g', 'Dry_Dead_g', 'Dry_Green_g', 'Dry_Total_g', 'GDM_g']\n",
      "ğŸŒ¿ Cleaning species names ...\n",
      "\n",
      "ğŸ“Š Species frequency:\n",
      "                Count  count\n",
      "0              clover    207\n",
      "1            ryegrass    175\n",
      "2            phalaris     76\n",
      "3              fescue     38\n",
      "4             lucerne     22\n",
      "5         barleygrass     18\n",
      "6         silvergrass     11\n",
      "7            capeweed     11\n",
      "8          speargrass     11\n",
      "9           crumbweed     10\n",
      "10        whiteclover     10\n",
      "11         bromegrass      7\n",
      "12      subcloverlosa      5\n",
      "13  subcloverdalkeith      3\n",
      "14              mixed      2\n"
     ]
    }
   ],
   "source": [
    "df_train = load_and_prepare_train_df().copy()\n",
    "\n",
    "print(\"ğŸŒ¿ Cleaning species names ...\")\n",
    "df_train[\"Species\"] = df_train[\"Species\"].astype(str).str.lower().str.strip()\n",
    "\n",
    "# --- æŒ‰ä¸‹åˆ’çº¿åˆ†å‰²ï¼Œå¹¶å±•å¼€ ---\n",
    "all_species = (\n",
    "    df_train[\"Species\"]\n",
    "    .str.split(\"_\")         # åˆ†å‰²\n",
    "    .explode()              # å±•å¼€æˆæ–°è¡Œ\n",
    "    .str.strip()            # å»ç©ºæ ¼\n",
    ")\n",
    "\n",
    "# --- ç»Ÿè®¡å‡ºç°æ¬¡æ•° ---\n",
    "species_counts = (\n",
    "    all_species.value_counts()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"Species\", \"Species\": \"Count\"})\n",
    ")\n",
    "\n",
    "print(\"\\nğŸ“Š Species frequency:\")\n",
    "print(species_counts)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
